{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpm4QsNInJ5gelfo1WWvwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrieVarshini2004/Taxi_Tip_Prediction/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlLh_c_2wC-U",
        "outputId": "165e7474-a171-4e83-acff-bbb99924ece6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "There are 6971560 observations in the dataset.\n",
            "There are 19 variables in the dataset.\n",
            "[Scikit-Learn] Training time (s): 9.32961\n",
            "[Snap ML] Training time (s): 4.07858\n",
            "[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup: 2.29 x\n",
            "[Scikit-Learn] MSE: 1.635\n",
            "[Snap ML] MSE: 1.611\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "import warnings\n",
        "import gc, os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/My Drive/yellow_tripdata_2019-06.parquet'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"⚠️ File not found at {file_path}. Uploading manually...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # Prompt user to upload file\n",
        "    file_path = 'yellow_tripdata_2019-06.parquet'  # Set new path\n",
        "\n",
        "# Read the Parquet file correctly\n",
        "raw_data = pd.read_parquet(file_path)\n",
        "\n",
        "# Print dataset info\n",
        "print(f\"There are {len(raw_data)} observations in the dataset.\")\n",
        "print(f\"There are {len(raw_data.columns)} variables in the dataset.\")\n",
        "\n",
        "# Data cleaning\n",
        "raw_data = raw_data[raw_data['tip_amount'] > 0]  # Remove zero-tip trips\n",
        "raw_data = raw_data[raw_data['tip_amount'] <= raw_data['fare_amount']]  # Remove unrealistic tips\n",
        "raw_data = raw_data[(raw_data['fare_amount'] >= 2) & (raw_data['fare_amount'] < 200)]  # Filter valid fares\n",
        "clean_data = raw_data.drop(['total_amount'], axis=1)  # Drop total_amount column\n",
        "del raw_data\n",
        "gc.collect()  # Free memory\n",
        "\n",
        "# Convert timestamps to datetime objects\n",
        "clean_data['tpep_dropoff_datetime'] = pd.to_datetime(clean_data['tpep_dropoff_datetime'])\n",
        "clean_data['tpep_pickup_datetime'] = pd.to_datetime(clean_data['tpep_pickup_datetime'])\n",
        "\n",
        "# Extract time-based features\n",
        "clean_data['pickup_hour'] = clean_data['tpep_pickup_datetime'].dt.hour\n",
        "clean_data['dropoff_hour'] = clean_data['tpep_dropoff_datetime'].dt.hour\n",
        "clean_data['pickup_day'] = clean_data['tpep_pickup_datetime'].dt.weekday\n",
        "clean_data['dropoff_day'] = clean_data['tpep_dropoff_datetime'].dt.weekday\n",
        "clean_data['trip_time'] = (clean_data['tpep_dropoff_datetime'] - clean_data['tpep_pickup_datetime']).dt.total_seconds()\n",
        "\n",
        "# Reduce dataset size (optional)\n",
        "clean_data = clean_data.head(200000)\n",
        "\n",
        "# Drop datetime columns\n",
        "clean_data = clean_data.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "categorical_columns = [\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\",\n",
        "                        \"PULocationID\", \"DOLocationID\", \"payment_type\",\n",
        "                        \"pickup_hour\", \"dropoff_hour\", \"pickup_day\", \"dropoff_day\"]\n",
        "proc_data = pd.get_dummies(clean_data, columns=categorical_columns)\n",
        "\n",
        "# Free memory\n",
        "del clean_data\n",
        "gc.collect()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "proc_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract target variable\n",
        "y = proc_data[['tip_amount']].values.astype('float32')\n",
        "proc_data = proc_data.drop(['tip_amount'], axis=1)\n",
        "\n",
        "# Convert feature matrix to NumPy array and normalize\n",
        "X = normalize(proc_data.values, axis=1, norm='l1', copy=False)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Scikit-Learn\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "sklearn_dt = DecisionTreeRegressor(max_depth=8, random_state=35)\n",
        "t0 = time.time()\n",
        "sklearn_dt.fit(X_train, y_train)\n",
        "sklearn_time = time.time() - t0\n",
        "print(\"[Scikit-Learn] Training time (s):\", round(sklearn_time, 5))\n",
        "\n",
        "# Train Decision Tree using Snap ML\n",
        "from snapml import DecisionTreeRegressor\n",
        "snapml_dt = DecisionTreeRegressor(max_depth=8, random_state=45, n_jobs=4)\n",
        "t0 = time.time()\n",
        "snapml_dt.fit(X_train, y_train)\n",
        "snapml_time = time.time() - t0\n",
        "print(\"[Snap ML] Training time (s):\", round(snapml_time, 5))\n",
        "\n",
        "# Training speedup comparison\n",
        "print('[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup:', round(sklearn_time / snapml_time, 2), \"x\")\n",
        "\n",
        "# Evaluate models\n",
        "sklearn_pred = sklearn_dt.predict(X_test)\n",
        "snapml_pred = snapml_dt.predict(X_test)\n",
        "print('[Scikit-Learn] MSE:', round(mean_squared_error(y_test, sklearn_pred), 3))\n",
        "print('[Snap ML] MSE:', round(mean_squared_error(y_test, snapml_pred), 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "print(os.listdir('/content/drive/My Drive/'))\n",
        "print(os.listdir('/content/drive/My Drive/Colab Notebooks/'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz3ZX2H9Aw5D",
        "outputId": "a5db44df-eaa8-4799-8e4a-d13a96c5114a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['Shrie_Varshini__-_ (2).pdf', 'Shrie_Varshini__-_ (1).pdf', 'Shrie_Varshini__-_.pdf', 'shrievarshini_cce.pdf', 'WhatsApp Image 2024-07-09 at 17.38.31_3f6d6046.jpg', 'Colab Notebooks', 'Future Tech AI Hackathon - Presentation Template.pptx', 'Advitiya (1).pdf', 'Advitiya.pdf', 'kidney_ct_scan_model', 'archive (1)', 'symbipredict_2022.csv', 'yellow_tripdata_2019-06.parquet', 'cell_samples.csv']\n",
            "['project initial.ipynb', 'diffusion.ipynb', 'KNN', 'Linear Regression.ipynb', 'Untitled0.ipynb', 'GAN', 'kidney_disease_det.ipynb', 'Untitled1.ipynb', 'creditcard fraud.ipynb', 'Logistic regression.ipynb', 'SVM.ipynb', 'multiclass_prediction.ipynb', 'GridsearchCV.ipynb', 'k-means.ipynb', 'Untitled2.ipynb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snapml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8MZr0vMMGPz",
        "outputId": "8b3ce6f9-4649-4a8d-e0c9-229495a73f23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snapml\n",
            "  Downloading snapml-1.16.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from snapml) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from snapml) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from snapml) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->snapml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->snapml) (3.5.0)\n",
            "Downloading snapml-1.16.2-cp311-cp311-manylinux_2_28_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snapml\n",
            "Successfully installed snapml-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Snap ML (only needed in Google Colab)\n",
        "!pip install snapml\n",
        "\n",
        "# Import necessary libraries\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "import warnings\n",
        "import gc, os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/My Drive/yellow_tripdata_2019-06.parquet'\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"⚠️ File not found at {file_path}. Uploading manually...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # Prompt user to upload file\n",
        "    file_path = 'yellow_tripdata_2019-06.parquet'  # Set new path\n",
        "\n",
        "# Read the Parquet file correctly\n",
        "raw_data = pd.read_parquet(file_path)\n",
        "\n",
        "# Print dataset info\n",
        "print(f\"There are {len(raw_data)} observations in the dataset.\")\n",
        "print(f\"There are {len(raw_data.columns)} variables in the dataset.\")\n",
        "\n",
        "# Data cleaning\n",
        "raw_data = raw_data[raw_data['tip_amount'] > 0]  # Remove zero-tip trips\n",
        "raw_data = raw_data[raw_data['tip_amount'] <= raw_data['fare_amount']]  # Remove unrealistic tips\n",
        "raw_data = raw_data[(raw_data['fare_amount'] >= 2) & (raw_data['fare_amount'] < 200)]  # Filter valid fares\n",
        "clean_data = raw_data.drop(['total_amount'], axis=1)  # Drop total_amount column\n",
        "del raw_data\n",
        "gc.collect()  # Free memory\n",
        "\n",
        "# Convert timestamps to datetime objects\n",
        "clean_data['tpep_dropoff_datetime'] = pd.to_datetime(clean_data['tpep_dropoff_datetime'])\n",
        "clean_data['tpep_pickup_datetime'] = pd.to_datetime(clean_data['tpep_pickup_datetime'])\n",
        "\n",
        "# Extract time-based features\n",
        "clean_data['pickup_hour'] = clean_data['tpep_pickup_datetime'].dt.hour\n",
        "clean_data['dropoff_hour'] = clean_data['tpep_dropoff_datetime'].dt.hour\n",
        "clean_data['pickup_day'] = clean_data['tpep_pickup_datetime'].dt.weekday\n",
        "clean_data['dropoff_day'] = clean_data['tpep_dropoff_datetime'].dt.weekday\n",
        "clean_data['trip_time'] = (clean_data['tpep_dropoff_datetime'] - clean_data['tpep_pickup_datetime']).dt.total_seconds()\n",
        "\n",
        "# Reduce dataset size (optional)\n",
        "clean_data = clean_data.head(200000)\n",
        "\n",
        "# Drop datetime columns\n",
        "clean_data = clean_data.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "categorical_columns = [\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\",\n",
        "                        \"PULocationID\", \"DOLocationID\", \"payment_type\",\n",
        "                        \"pickup_hour\", \"dropoff_hour\", \"pickup_day\", \"dropoff_day\"]\n",
        "proc_data = pd.get_dummies(clean_data, columns=categorical_columns)\n",
        "\n",
        "# Free memory\n",
        "del clean_data\n",
        "gc.collect()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "proc_data.fillna(0, inplace=True)\n",
        "\n",
        "# Extract target variable\n",
        "y = proc_data[['tip_amount']].values.astype('float32')\n",
        "proc_data = proc_data.drop(['tip_amount'], axis=1)\n",
        "\n",
        "# Convert feature matrix to NumPy array and normalize\n",
        "X = normalize(proc_data.values, axis=1, norm='l1', copy=False)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree using Scikit-Learn\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "sklearn_dt = DecisionTreeRegressor(max_depth=8, random_state=35)\n",
        "t0 = time.time()\n",
        "sklearn_dt.fit(X_train, y_train)\n",
        "sklearn_time = time.time() - t0\n",
        "print(\"[Scikit-Learn] Training time (s):\", round(sklearn_time, 5))\n",
        "\n",
        "# Train Decision Tree using Snap ML\n",
        "from snapml import DecisionTreeRegressor\n",
        "snapml_dt = DecisionTreeRegressor(max_depth=8, random_state=45, n_jobs=4)\n",
        "t0 = time.time()\n",
        "snapml_dt.fit(X_train, y_train)\n",
        "snapml_time = time.time() - t0\n",
        "print(\"[Snap ML] Training time (s):\", round(snapml_time, 5))\n",
        "\n",
        "# Training speedup comparison\n",
        "print('[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup:', round(sklearn_time / snapml_time, 2), \"x\")\n",
        "\n",
        "# Evaluate models\n",
        "sklearn_pred = sklearn_dt.predict(X_test)\n",
        "snapml_pred = snapml_dt.predict(X_test)\n",
        "print('[Scikit-Learn] MSE:', round(mean_squared_error(y_test, sklearn_pred), 3))\n",
        "print('[Snap ML] MSE:', round(mean_squared_error(y_test, snapml_pred), 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcCtxeyOTTU",
        "outputId": "c3bd437d-69af-4125-fc35-44b6da40c8a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snapml in /usr/local/lib/python3.11/dist-packages (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from snapml) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from snapml) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from snapml) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->snapml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->snapml) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "There are 6971560 observations in the dataset.\n",
            "There are 19 variables in the dataset.\n",
            "[Scikit-Learn] Training time (s): 9.05572\n",
            "[Snap ML] Training time (s): 5.19263\n",
            "[Decision Tree Regressor] Snap ML vs. Scikit-Learn speedup: 1.74 x\n",
            "[Scikit-Learn] MSE: 1.635\n",
            "[Snap ML] MSE: 1.611\n"
          ]
        }
      ]
    }
  ]
}